
# ---- ##  Experiment Setup, Logging Specifications, Environment  ## ---- #
experiment:
  description: |
    > Reimplementation of BCV for 3D segmentation finetuning.
    >   Data: 30 volumes of 13 abdominal structures.
    >    9/4 Best Settings: Gen-UNet, PGLscales (1.1, 1.4), wCE ('bcv'), bbgdc
    >    9/10 Use nnUNet3d Settings: fixed sampling, network, optim, deep-sup.
  id: "(0915bcv-3x)TUNE_nnunet3d_nesterov_cedc_s3"     # for wandb run identification

  name: 'ftbcv'   # for experiment module identification
  project: '3DSeg-Pretrain'

  debug:                                            # (Correct Alignment)
    mode: False                                     # < Check
    overfitbatch: False                             # < Check
    wandb: True                                     # < Check
    save: True                                      # < Check
    break_train_iter: False                         # < Check
    break_test_iter: False                          # < Check
    test_every_n_epochs: 1
  seed: 26
  hpsamp: ""

  checkpoint:
    file: '' #  '(0901)bcv-overfit_genunet_adam_cew+dice-batch_s3_ftbcv_ep499_last.pth'


# ---- ##  Data Preprocessing, Loading, Transformation  ## ---- #

data:  # describing project-wide data specifications
  name: 'bcv'
  kits18: ''
  msd: ''
  bcv:
    num_classes: 14
    split: '60-20-20_3.csv'


# Model / Criterion / Other nn.Module Specifications
model:
  name: 'nnunet3d'  # 'genesis_unet3d'  # gn_unet3d
  init: 'kaiming'
  sync_bn: True



# ---- ##  Training: iterations, criterion, optimization, etc.  ## ---- #

train:
  deep_sup: True

  epochs: 200
  batch_size: 2
  examples_per_volume: 100  # 250 iterations per epoch (nnUNet)
  num_workers: 2

  patch_size: [32, 176, 176]
  fg_bias: 0.5
  scale_range: [[0.7, 1.4], [0.7, 1.4], [0.7, 1.4]]

  criterion:
    name: 'dice_ce_nnunet'
    dice_ce_nnunet:
      dc_kw:
        do_bg: True
        batch_dice: True
        smooth: 1.
      ce_kw:
        weights_key: ''  # 'bcv' # 'bcv_cbrt'
    soft_dice_nnunet:
      do_bg: null

  optimizer:
    name: 'nesterov'
    lr: 0.01
    
    nesterov:
      momentum: 0.99
    adam:
      betas: [0.9, 0.999]
    sgd:
      momentum: 0.9

  scheduler:
    name: 'poly'
    rampup_rates: []
    min_lr: 0.000001
    
    poly:
      power: 0.9

    step:
      factor: 0.1
      steps: [0.5, 0.8]
    
    exponential:
      exp_factor: 0.95

test:
  batch_size: 2




