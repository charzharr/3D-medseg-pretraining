  # ---- Pretrained Experiment Names --- #
  # 0221 MGv2 Experiments
  #   (0221_r2u101_pretr_mg-mse+!b)ep200_adamw,001-wd0-cos_mse_r1_prevec
  #   (0221_r2u101_pretr_mg-l1+!b)ep200_adamw,001-wd0-cos_mse_r1
  #   (0221_r2u101_pretr_mg-l1+l1)ep200_adamw,0002-wd0-cos_mse_r1
  #   (0221_r2u101_pretr_mg-l1+l1full)ep200_adamw,0002-wd0-cos_mse_r1
  #   (0221_r2u101_pretr_mg-mse+l1full)ep200_adamw,0002-wd0-cos_mse_r1
  # 0219 Rubik (2set): (0219_r2u101_pretr_rubik-n7m4)ep300_adamw,0001-wd0-cos_mse_r1
  #   * (0219_r2u101_pretr_rubik-n7m4)ep300_adamw,0001-wd0-cos_mse_r1
  # 0219 SAR (2lrs): (0219_r2u101_pretr_sar)ep300_adamw,0001-wd0-cos_mse_r1
  #   * (0219_r2u101_pretr_sar)ep300_adamw,0001-wd0-cos_mse_r1
  # 0219 MG (2lrs): (0219_r2u101_pretr_mg)ep300_adamw,002-wd0-cos_mse_r1
  # 0217 CV+1: (0217_r2u101_pretr_CV+1)flip_vs100_adamw,0025-cos_mse_r1
  # 0217 CV+2: (0217_r2u101_pretr_CV+2)flip_vs100_adamw,0025-cos_mse_r1


# ---- ##  Experiment Setup, Logging Specifications, Environment  ## ---- #
experiment:
  description: |
    > Reimplementation of BCV for 3D segmentation finetuning.
    >   Data: 30 volumes of 13 abdominal structures.
    >    9/4 Best Settings: Gen-UNet, PGLscales (1.1, 1.4), wCE ('bcv'), bbgdc
    >    9/10 Use nnUNet3d Settings: fixed sampling, network, optim, deep-sup.
  id: "(0226_bcvtune-fs100_0219sar-lr,0001)vs50_dsup+batchdice_cedc-adamw,001-cos_r1"     # for wandb run identification

  name: 'ftbcv'   # for experiment module identification
  project: 'miccai22_pretrain3d'

  debug:                                            # (Correct Alignment)
    mode: False                                     # < Check
    overfitbatch: False                             # < Check
    wandb: True                                     # < Check
    save: False                                      # < Check
    break_train_iter: False                         # < Check
    break_test_iter: False                          # < Check
    test_every_n_epochs: 1
  seed: 420
  hpsamp: ""
  adjust_by_comp: False  # adjust training params based on specific components

  checkpoint:
    file: '(0219_r2u101_pretr_sar)ep300_adamw,0001-wd0-cos_mse_r1'


# ---- ##  Data Preprocessing, Loading, Transformation  ## ---- #

data:  # describing project-wide data specifications
  name: 'bcv'
  kits18: ''
  msd: ''
  bcv:
    num_classes: 14
    split: 'cotr70-30_fs100.csv'  # 60-20-20.csv, cotr70-30_fs100.csv 'cotr70-30_fs50_a.csv'


# Model / Criterion / Other nn.Module Specifications
model:                # 'nnunet3d'  # 'genesis_unet3d'  # gn_unet3d
  name: 'res2unet3d'  # nnunet3d, hrnet3d, denseunet3d, unet3p3d, res2unet3d
  init: 'kaiming'  
  sync_bn: True
  norm: 'batchnorm'
  act: 'relu'

  denseunet3d:
    layers: 201
  hrnet3d:
    cfg: 'seg_hrnetv2_w48.yaml'
  res2unet3d:
    layers: 101
    base_width: 26  # default: 26



# ---- ##  Training: iterations, criterion, optimization, etc.  ## ---- #

train:
  deep_sup: True

  epochs: 100
  start_epoch: 0
  batch_size: 4   # subject to change based on model, deep_sup, crop size
  examples_per_volume: 50  # 250, 250 iterations per epoch (nnUNet)
    # for fs100: 
  num_workers: 3

  patch_size: [64, 128, 128] # [64, 128, 128], [32, 160, 160]
  fg_bias: 0.333
  scale_range: [0.9, 1.1]

  t_flip: 0.25  # flip on all 3 axis
  t_gn: 0.  # gaussian noise
  t_gb: 0.15  # gaussian blur
  t_intensity_scale: 0.25
  t_gamma: 0.15

  criterion:
    name: 'dice_ce_nnunet'
    dice_ce_nnunet:
      dc_kw:
        do_bg: True
        batch_dice: True
        smooth: 1.
      ce_kw:
        weights_key: ''  # 'bcv' # 'bcv_cbrt'
    soft_dice_nnunet:
      do_bg: null

  optimizer:
    name: 'adamw'
    lr: 0.001  # usually 0.01 for FS50 & above
    wt_decay: 0.01
    
    nesterov:
      momentum: 0.99
    sgd:
      momentum: 0.9
    adam:
      betas: [0.9, 0.999]
    adamw:
      betas: [0.9, 0.999]

  scheduler:
    name: 'cosine'
    rampup_rates: []
    min_lr: 0.000001
    
    poly:
      power: 0.9

    step:
      factor: 0.2
      steps: [0.3, 0.6, 0.9]
    
    exponential:
      exp_factor: 0.95

test:
  batch_size: 4




