{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historic-convergence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:42:12.986002Z",
     "iopub.status.busy": "2021-09-01T08:42:12.985377Z",
     "iopub.status.idle": "2021-09-01T08:42:14.537764Z",
     "shell.execute_reply": "2021-09-01T08:42:14.536617Z",
     "shell.execute_reply.started": "2021-09-01T08:42:12.985870Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## General Imports from all libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import pathlib\n",
    "import glob\n",
    "import time\n",
    "import math, random\n",
    "import pprint\n",
    "import collections\n",
    "import numbers, string\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "curr_path = pathlib.Path(os.getcwd()).absolute()\n",
    "\n",
    "cards = !echo $SGE_HGR_gpu_card\n",
    "device = torch.device(f\"cuda:{cards[0]}\" if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amber-surge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:42:14.540015Z",
     "iopub.status.busy": "2021-09-01T08:42:14.539643Z",
     "iopub.status.idle": "2021-09-01T08:42:14.577739Z",
     "shell.execute_reply": "2021-09-01T08:42:14.576820Z",
     "shell.execute_reply.started": "2021-09-01T08:42:14.539977Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding current path: /afs/crc.nd.edu/user/y/yzhang46/_3DPRE/src/pengfei_inference\n"
     ]
    }
   ],
   "source": [
    "# Import custom files for this project\n",
    "if curr_path not in sys.path:\n",
    "    print('Adding current path:', curr_path)\n",
    "    sys.path.append(str(curr_path))\n",
    "\n",
    "from metrics import batch_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-narrow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T05:34:03.174227Z",
     "iopub.status.busy": "2021-09-01T05:34:03.173579Z",
     "iopub.status.idle": "2021-09-01T05:34:03.240663Z",
     "shell.execute_reply": "2021-09-01T05:34:03.239433Z",
     "shell.execute_reply.started": "2021-09-01T05:34:03.174147Z"
    },
    "tags": []
   },
   "source": [
    "# Data Collection and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressed-terry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:42:15.083424Z",
     "iopub.status.busy": "2021-09-01T08:42:15.082905Z",
     "iopub.status.idle": "2021-09-01T08:42:15.137992Z",
     "shell.execute_reply": "2021-09-01T08:42:15.136940Z",
     "shell.execute_reply.started": "2021-09-01T08:42:15.083375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_files, mask_files, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.image_files = image_files  # list of image files\n",
    "        self.mask_files = mask_files  # list of mask files\n",
    "        \n",
    "        # Get image tensors and store permanently\n",
    "        self.images, self.masks, self.image_info = [], [], []\n",
    "        for image_f, mask_f in zip(image_files, mask_files):\n",
    "            # 1. Read image and preprocess (clamp + normalize)\n",
    "            sitk_image = sitk.ReadImage(image_f, sitk.sitkInt16)\n",
    "            sitk_image = sitk.Clamp(sitk_image, sitk.sitkInt16, -1024, 325)\n",
    "            sitk_image = sitk.NormalizeImageFilter().Execute(sitk_image)\n",
    "            image_tensor = torch.from_numpy(sitk.GetArrayFromImage(sitk_image))\n",
    "            image_tensor = image_tensor.float()\n",
    "            \n",
    "            # 2. Read mask and convert it to one-hot\n",
    "            sitk_mask = sitk.ReadImage(mask_f, sitk.sitkInt64)\n",
    "            mask_tensor = torch.from_numpy(sitk.GetArrayFromImage(sitk_mask))\n",
    "            \n",
    "            shape = image_tensor.shape\n",
    "            oh_shape = [num_classes] + list(shape)\n",
    "            mask_oh_tensor = torch.zeros(oh_shape, dtype=torch.int32)\n",
    "            mask_oh_tensor.scatter_(0, mask_tensor.unsqueeze(0), 1)\n",
    "            \n",
    "            self.images.append(image_tensor)\n",
    "            self.masks.append(mask_oh_tensor)\n",
    "            self.image_info.append({\n",
    "                'origin': sitk_image.GetOrigin(),\n",
    "                'spacing': sitk_image.GetSpacing(),\n",
    "                'direction': sitk_image.GetDirection()\n",
    "            })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        info_d = self.image_info[idx]\n",
    "        return image, mask, info_d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-capacity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:42:15.499854Z",
     "iopub.status.busy": "2021-09-01T08:42:15.499324Z",
     "iopub.status.idle": "2021-09-01T08:42:34.999120Z",
     "shell.execute_reply": "2021-09-01T08:42:34.998075Z",
     "shell.execute_reply.started": "2021-09-01T08:42:15.499806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 19.44656014442444 sec to load 6 test images.\n"
     ]
    }
   ],
   "source": [
    "bcv_dir = pathlib.Path('/afs/crc.nd.edu/user/y/yzhang46/datasets/BCV-2015')\n",
    "train_image_dir = bcv_dir / 'train' / 'img_nii'\n",
    "train_mask_dir = bcv_dir / 'train' / 'label_nii'\n",
    "\n",
    "images = sorted(glob.glob(str(train_image_dir) + '/*.nii.gz'))\n",
    "masks = sorted(glob.glob(str(train_mask_dir) + '/*.nii.gz'))\n",
    "\n",
    "start = time.time()\n",
    "test_size = 6\n",
    "num_classes = 14\n",
    "test_set = MockDataset(images[:test_size], masks[:test_size], num_classes)\n",
    "print(f'Took {time.time() - start} sec to load {test_size} test images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-kuwait",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T05:34:44.322598Z",
     "iopub.status.busy": "2021-09-01T05:34:44.321994Z",
     "iopub.status.idle": "2021-09-01T05:34:44.386440Z",
     "shell.execute_reply": "2021-09-01T05:34:44.385229Z",
     "shell.execute_reply.started": "2021-09-01T05:34:44.322544Z"
    },
    "tags": []
   },
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contained-velvet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:42:35.000963Z",
     "iopub.status.busy": "2021-09-01T08:42:35.000695Z",
     "iopub.status.idle": "2021-09-01T08:42:35.034032Z",
     "shell.execute_reply": "2021-09-01T08:42:35.033253Z",
     "shell.execute_reply.started": "2021-09-01T08:42:35.000936Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-sixth",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hydraulic-development",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T08:44:06.402128Z",
     "iopub.status.busy": "2021-09-01T08:44:06.401546Z",
     "iopub.status.idle": "2021-09-01T08:46:42.171456Z",
     "shell.execute_reply": "2021-09-01T08:46:42.169954Z",
     "shell.execute_reply.started": "2021-09-01T08:44:06.402076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⭐ Inference for example 1..\n",
      "Batch 1 / 4\n",
      "Batch 2 / 4\n",
      "Batch 3 / 4\n",
      "Batch 4 / 4\n",
      "Aggregate (divide):  0.5069081783294678\n",
      "Aggregate:  2.1341826915740967\n",
      "Getting image metrics..\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mets time: 16.65\n",
      "Saving image..\n",
      "Completed inference for test volume 1 (40.24).\n",
      "⭐ Inference for example 2..\n",
      "Batch 1 / 4\n",
      "Batch 2 / 4\n",
      "Batch 3 / 4\n",
      "Batch 4 / 4\n",
      "Aggregate (divide):  0.4191896915435791\n",
      "Aggregate:  1.9012672901153564\n",
      "Getting image metrics..\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mets time: 15.83\n",
      "Saving image..\n",
      "Completed inference for test volume 2 (39.73).\n",
      "⭐ Inference for example 3..\n",
      "Batch 1 / 6\n",
      "Batch 2 / 6\n",
      "Batch 3 / 6\n",
      "Batch 4 / 6\n",
      "Batch 5 / 6\n",
      "Batch 6 / 6\n",
      "Aggregate (divide):  0.5384798049926758\n",
      "Aggregate:  3.03458571434021\n",
      "Getting image metrics..\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1.0 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mets time: 22.13\n",
      "Saving image..\n",
      "Completed inference for test volume 3 (56.08).\n",
      "⭐ Inference for example 4..\n",
      "Batch 1 / 4\n",
      "Batch 2 / 4\n",
      "Batch 3 / 4\n",
      "Batch 4 / 4\n",
      "Aggregate (divide):  0.37314462661743164\n",
      "Aggregate:  1.8841361999511719\n",
      "Getting image metrics..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4c7e3f5e6889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Getting image metrics..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m# preds are CxDxHxW, but batch input takes 1xCxDxHxW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_3DPRE/src/pengfei_inference/metrics.py\u001b[0m in \u001b[0;36mbatch_metrics\u001b[0;34m(preds, targs, ignore_background, naive_avg)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     cdj_tuple = batch_cdj_metrics(preds, targs,\n\u001b[0;32m---> 41\u001b[0;31m                                   ignore_background=ignore_background)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ConfusionMatrix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'tp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_3DPRE/src/pengfei_inference/metrics.py\u001b[0m in \u001b[0;36mbatch_cdj_metrics\u001b[0;34m(pred, targ, ignore_background)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{pred.shape} {targ.shape} mismatch!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_background\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_background\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtp\u001b[0m  \u001b[0;31m# BxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/_3DPRE/src/pengfei_inference/metrics.py\u001b[0m in \u001b[0;36mbatch_confusion_matrix\u001b[0;34m(pred, targ, ignore_background)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_flat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarg_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BxCxS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_flat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarg_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BxCxS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muni_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from inference import ChopBatchAggregate3d as CBA\n",
    "\n",
    "num_classes = 14\n",
    "device = device\n",
    "dataset = test_set\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(dataset)):\n",
    "        start_vol = time.time()\n",
    "        print(f'⭐ Inference for example {i+1}..')\n",
    "        image, mask, info = dataset[i]  # image: DxHxW, mask: CxDxHxW\n",
    "                                        #  image: float32, mask: int32\n",
    "        \n",
    "        # Create Chop, batch, aggregator object\n",
    "        image = image   # faster calc with GPUs\n",
    "        mask = mask\n",
    "        cba = CBA(image, (48, 160, 160), (20, 20, 20), 20, num_classes)\n",
    "        \n",
    "        # Run inference on batches of crops of image\n",
    "        for bidx, batch in enumerate(cba):\n",
    "            print(f'Batch {bidx + 1} / {len(cba)}')\n",
    "            crops, locations = batch\n",
    "            crops = crops.to(device)\n",
    "            \n",
    "            # NOTE: In real code replace with logits = model(crops)\n",
    "            # NOTE: I simulate logits here in 2 ways\n",
    "            #  1. I take random floats\n",
    "            use_mask_as_logits = True\n",
    "            if use_mask_as_logits:\n",
    "                logits_shape = [crops.shape[0], num_classes] + list(crops.shape[2:])\n",
    "                logits = torch.zeros(logits_shape, device=image.device)\n",
    "                for n in range(locations.shape[0]):\n",
    "                    lower = locations[n, :3]\n",
    "                    upper = locations[n, 3:]\n",
    "                    logits[n] = mask[:, lower[0]:upper[0],\n",
    "                                     lower[1]:upper[1],\n",
    "                                     lower[2]:upper[2]]\n",
    "            #  2. I feed in the gt mask itself as logits (should return 100% acc)\n",
    "            else:\n",
    "                logits = torch.randn(logits_shape, device=image.device) \n",
    "    \n",
    "            cba.add_batch_predictions(logits.cpu(), locations, act='none')\n",
    "                # NOTE: in this case, we are averaging logits, if you want\n",
    "                #  to average probabilities instead, use act='softmax'\n",
    "        \n",
    "        # Get final predictions, calculate metrics\n",
    "        agg_predictions = cba.aggregate(ret='one_hot', cpu=True, numpy=False)\n",
    "        \n",
    "        print(f'Getting image metrics..')\n",
    "        start = time.time()\n",
    "        mets = batch_metrics(agg_predictions.unsqueeze(0), mask.unsqueeze(0))\n",
    "            # preds are CxDxHxW, but batch input takes 1xCxDxHxW\n",
    "        print(mets['dice_mean'], mets['dice_class'])\n",
    "        print(mets['jaccard_mean'], mets['jaccard_class'])\n",
    "        print(f'Mets time: {time.time() - start:.2f}')\n",
    "        \n",
    "        # Convert from 1hot to id and save prediction volume\n",
    "        print(f'Saving image..')\n",
    "        id_preds = agg_predictions.argmax(0).numpy().astype(np.uint16)\n",
    "        sitk_pred = sitk.GetImageFromArray(id_preds, isVector=False)\n",
    "        sitk_pred.SetOrigin(info['origin'])\n",
    "        sitk_pred.SetSpacing(info['spacing'])\n",
    "        sitk_pred.SetDirection(info['direction'])\n",
    "        sitk.WriteImage(sitk_pred, 'prediction.nii.gz')\n",
    "        \n",
    "        elapsed = time.time() - start_vol\n",
    "        print(f'Completed inference for test volume {i+1} ({elapsed:.2f}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
